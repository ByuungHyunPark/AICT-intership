{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "from konlpy.tag import Kkma\n",
    "from eunjeon import Mecab\n",
    "\n",
    "from tqdm.notebook import tqdm\n",
    "\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.metrics import classification_report, confusion_matrix, accuracy_score, make_scorer\n",
    "from sklearn.model_selection import train_test_split, cross_val_score, KFold, cross_validate, StratifiedKFold\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "from gensim.models.doc2vec import Doc2Vec, TaggedDocument"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>시각</th>\n",
       "      <th>카테고리</th>\n",
       "      <th>미디어</th>\n",
       "      <th>제목</th>\n",
       "      <th>내용</th>\n",
       "      <th>url</th>\n",
       "      <th>수정시간</th>\n",
       "      <th>감정표현</th>\n",
       "      <th>timestamp</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>7987205</th>\n",
       "      <td>2020.12.31. 오전 12:00</td>\n",
       "      <td>사회</td>\n",
       "      <td>동아일보</td>\n",
       "      <td>경기도 공공배달앱 ‘배달특급’ 20여일만에 거래액 20억 돌파</td>\n",
       "      <td>낮은 수수료로 배달매출에 기댄 소상공인들의 부담 확 덜어줘 경기 오산시에서 중국집을...</td>\n",
       "      <td>https://news.naver.com/main/read.nhn?mode=LSD&amp;...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>[4, 0, 0, 1, 0]</td>\n",
       "      <td>1.610066e+09</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7987206</th>\n",
       "      <td>2020.12.31. 오전 12:00</td>\n",
       "      <td>사회</td>\n",
       "      <td>뉴시스</td>\n",
       "      <td>녹유 오늘의 운세51년생 침이 고여지는 대접을 받아요</td>\n",
       "      <td>서울 뉴시스 녹유 錄喩 의 오늘의 운세 2020년 12월 31일 목요일 음력 11월...</td>\n",
       "      <td>https://news.naver.com/main/read.nhn?mode=LSD&amp;...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>[0, 0, 0, 0, 0]</td>\n",
       "      <td>1.610066e+09</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                           시각 카테고리   미디어                                  제목  \\\n",
       "7987205  2020.12.31. 오전 12:00   사회  동아일보  경기도 공공배달앱 ‘배달특급’ 20여일만에 거래액 20억 돌파   \n",
       "7987206  2020.12.31. 오전 12:00   사회   뉴시스       녹유 오늘의 운세51년생 침이 고여지는 대접을 받아요   \n",
       "\n",
       "                                                        내용  \\\n",
       "7987205  낮은 수수료로 배달매출에 기댄 소상공인들의 부담 확 덜어줘 경기 오산시에서 중국집을...   \n",
       "7987206  서울 뉴시스 녹유 錄喩 의 오늘의 운세 2020년 12월 31일 목요일 음력 11월...   \n",
       "\n",
       "                                                       url 수정시간  \\\n",
       "7987205  https://news.naver.com/main/read.nhn?mode=LSD&...  NaN   \n",
       "7987206  https://news.naver.com/main/read.nhn?mode=LSD&...  NaN   \n",
       "\n",
       "                    감정표현     timestamp  \n",
       "7987205  [4, 0, 0, 1, 0]  1.610066e+09  \n",
       "7987206  [0, 0, 0, 0, 0]  1.610066e+09  "
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "naverNews = pd.read_csv('data/naverNewsFinal.csv')\n",
    "\n",
    "naverNews['감정표현'] = naverNews['감정표현'].apply(lambda x : [int(i) for i in x[1:-1].split(', ')])\n",
    "\n",
    "naverNews.tail(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: 'final_dataset.csv'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-3-bf7b0bb34aaa>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mmergeDF\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mread_csv\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'final_dataset.csv'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      2\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[0mmergeDF\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'감정표현'\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmergeDF\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'감정표현'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mapply\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;32mlambda\u001b[0m \u001b[0mx\u001b[0m \u001b[1;33m:\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0mint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mx\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m-\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m', '\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[0mmergeDF\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'sum'\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmergeDF\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'제보횟수'\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;34m'악의적헤드라인'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'헛소리선동'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'사실 및 통계왜곡'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msum\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0maxis\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\pandas\\io\\parsers.py\u001b[0m in \u001b[0;36mread_csv\u001b[1;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, squeeze, prefix, mangle_dupe_cols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, dialect, error_bad_lines, warn_bad_lines, delim_whitespace, low_memory, memory_map, float_precision)\u001b[0m\n\u001b[0;32m    684\u001b[0m     )\n\u001b[0;32m    685\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 686\u001b[1;33m     \u001b[1;32mreturn\u001b[0m \u001b[0m_read\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkwds\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    687\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    688\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\pandas\\io\\parsers.py\u001b[0m in \u001b[0;36m_read\u001b[1;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[0;32m    450\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    451\u001b[0m     \u001b[1;31m# Create the parser.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 452\u001b[1;33m     \u001b[0mparser\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mTextFileReader\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfp_or_buf\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    453\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    454\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mchunksize\u001b[0m \u001b[1;32mor\u001b[0m \u001b[0miterator\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\pandas\\io\\parsers.py\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, f, engine, **kwds)\u001b[0m\n\u001b[0;32m    944\u001b[0m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0moptions\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m\"has_index_names\"\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mkwds\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m\"has_index_names\"\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    945\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 946\u001b[1;33m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_make_engine\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mengine\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    947\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    948\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mclose\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\pandas\\io\\parsers.py\u001b[0m in \u001b[0;36m_make_engine\u001b[1;34m(self, engine)\u001b[0m\n\u001b[0;32m   1176\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m_make_engine\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mengine\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m\"c\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1177\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mengine\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;34m\"c\"\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1178\u001b[1;33m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_engine\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mCParserWrapper\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mf\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0moptions\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1179\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1180\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mengine\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;34m\"python\"\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\pandas\\io\\parsers.py\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, src, **kwds)\u001b[0m\n\u001b[0;32m   2006\u001b[0m         \u001b[0mkwds\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m\"usecols\"\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0musecols\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2007\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 2008\u001b[1;33m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_reader\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mparsers\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mTextReader\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msrc\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   2009\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0munnamed_cols\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_reader\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0munnamed_cols\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2010\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mpandas\\_libs\\parsers.pyx\u001b[0m in \u001b[0;36mpandas._libs.parsers.TextReader.__cinit__\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;32mpandas\\_libs\\parsers.pyx\u001b[0m in \u001b[0;36mpandas._libs.parsers.TextReader._setup_parser_source\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'final_dataset.csv'"
     ]
    }
   ],
   "source": [
    "mergeDF = pd.read_csv('final_dataset.csv')\n",
    "\n",
    "mergeDF['감정표현'] = mergeDF['감정표현'].apply(lambda x : [int(i) for i in x[1:-1].split(', ')])\n",
    "mergeDF['sum'] = mergeDF[['제보횟수','악의적헤드라인', '헛소리선동', '사실 및 통계왜곡']].sum(axis = 1)\n",
    "\n",
    "mergeDF.tail(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 데이터 셋 나누기\n",
    "- 1. 진짜뉴스 \n",
    "- 2. 가짜뉴스  (sum >= 3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('sum값 0의 개수 : ', len(mergeDF[mergeDF['sum']<1]))\n",
    "print('sum값 1이상 2이하 개수 : ', len(mergeDF[(mergeDF['sum']>=1)  & (mergeDF['sum']<9)]))\n",
    "print('sum값 3이상 개수 : ', len(mergeDF[mergeDF['sum']>=9]))\n",
    "\n",
    "neutral_news = mergeDF[(mergeDF['sum']>=1)  & (mergeDF['sum']<9)]\n",
    "fake_news = mergeDF[mergeDF['sum']>=3]\n",
    "\n",
    "print(neutral_news.shape, fake_news.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 언론사별 특징 파악"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "category_media_fake_cnt = mergeDF.groupby(['카테고리','미디어']).sum('사실 및 통계왜곡').sort_values(by = '사실 및 통계왜곡', ascending = False).reset_index()\n",
    "category_media_fake_cnt.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "media_naver_total_cnt = pd.DataFrame(naverNews[['미디어','카테고리']].value_counts()).reset_index().rename(columns = {0 : 'total_news_cnt'})\n",
    "media_naver_reportash_cnt = pd.DataFrame(mergeDF[['미디어','카테고리']].value_counts()).reset_index().rename(columns = {0 : 'reportash_cnt'})\n",
    "\n",
    "\n",
    "category_media_fake_cnt = pd.merge(category_media_fake_cnt, media_naver_total_cnt, how = 'left')\n",
    "category_media_fake_cnt = pd.merge(category_media_fake_cnt, media_naver_reportash_cnt, how = 'left')\n",
    "\n",
    "\n",
    "reportrash_rate = category_media_fake_cnt['reportash_cnt'] / category_media_fake_cnt['total_news_cnt'].values\n",
    "category_media_fake_cnt.loc[:, 'reportrash_rate(%)'] = reportrash_rate * 100\n",
    "category_media_fake_cnt= category_media_fake_cnt.sort_values(by = 'reportrash_rate(%)', ascending = False)\n",
    "category_media_fake_cnt= category_media_fake_cnt.drop('newsID', axis = 1 ).reset_index(drop = True)\n",
    "\n",
    "category_media_fake_cnt.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "low_rate_polictics_media = list(category_media_fake_cnt[category_media_fake_cnt['카테고리'] == '정치'].tail(5)['미디어'])\n",
    "low_rate_economy_media = list(category_media_fake_cnt[category_media_fake_cnt['카테고리'] == '경제'].tail(5)['미디어'])\n",
    "low_rate_social_media = list(category_media_fake_cnt[category_media_fake_cnt['카테고리'] == '사회'].tail(5)['미디어'])\n",
    "\n",
    "print(low_rate_polictics_media)\n",
    "print(low_rate_economy_media)\n",
    "print(low_rate_social_media)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "category_media_fake_cnt[category_media_fake_cnt['카테고리'] == '정치'].tail(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "category_media_fake_cnt[category_media_fake_cnt['카테고리'] == '경제'].tail(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "category_media_fake_cnt[category_media_fake_cnt['카테고리'] == '사회'].tail(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "naverNews['좋아요'] = naverNews['감정표현'].apply(lambda x : x[0])\n",
    "naverNews['훈훈해요'] = naverNews['감정표현'].apply(lambda x : x[1])\n",
    "naverNews['슬퍼요'] = naverNews['감정표현'].apply(lambda x : x[2])\n",
    "naverNews['화나요'] = naverNews['감정표현'].apply(lambda x : x[3])\n",
    "naverNews['후속기사원해요'] = naverNews['감정표현'].apply(lambda x : x[4])\n",
    "\n",
    "naverNews['감정표현sum'] = naverNews[['좋아요','훈훈해요','슬퍼요','화나요','후속기사원해요']].sum(axis = 1)\n",
    "\n",
    "naverNews.tail(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mergeDF[mergeDF['감정표현'].apply(lambda x : sum(x)) < 10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 진짜 정치뉴스 데이터의 후보군\n",
    "- [정치, 경제, 사회] 3가지의 카테고리로 따로 후보군 데이터프레이 생성\n",
    "    - 네이버뉴스 감정표현 합 10 이상 : 어느정도 이슈화된 뉴스를 선정 ex)날씨에 대한 뉴스, 주식 지수 관련뉴스 등 뉴스 제외\n",
    "    - 카테고리별로 reportrash에 제보된 비율이 낮은 언론사들의 뉴스를 활용\n",
    "    - 레포트래쉬에 제보된 뉴스는 제외"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "naverNews.tail(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#진짜 정치뉴스 데이터의 후보군\n",
    "real_politics_news = naverNews[(naverNews['카테고리'] == '정치')  & \\\n",
    "                               (naverNews['미디어'].isin(low_rate_polictics_media)) &\\\n",
    "                               (naverNews['감정표현sum'] > 10) &\\\n",
    "                               (~naverNews['제목'].isin(mergeDF['제목']))]\n",
    "\n",
    "\n",
    "#진짜 경제뉴스 데이터의 후보군\n",
    "real_economy_news = naverNews[(naverNews['카테고리'] == '경제')  & \\\n",
    "                              (naverNews['미디어'].isin(low_rate_economy_media)) &\\\n",
    "                              (naverNews['감정표현sum'] > 10) &\\\n",
    "                              (~naverNews['제목'].isin(mergeDF['제목']))]\n",
    "\n",
    "\n",
    "#진짜 사회뉴스 데이터의 후보군\n",
    "real_social_news = naverNews[(naverNews['카테고리'] == '사회')  & \\\n",
    "                             (naverNews['미디어'].isin(low_rate_social_media)) &\\\n",
    "                             (naverNews['감정표현sum'] > 10) &\\\n",
    "                             (~naverNews['제목'].isin(mergeDF['제목']))]\n",
    "\n",
    "\n",
    "real_news = pd.concat([real_politics_news, real_economy_news, real_social_news], ignore_index = True)\n",
    "\n",
    "\n",
    "print(real_politics_news.shape, real_economy_news.shape, real_social_news.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "\n",
    "rand_index_11500 = random.sample(range(0, real_news.shape[0]), 11500)\n",
    "\n",
    "real_news = real_news.iloc[rand_index_11500, :]\n",
    "print(real_news.shape)\n",
    "\n",
    "display(real_news.tail(2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "real_news.loc[:, 'label'] = 0        #진짜 : 0\n",
    "neutral_news.loc[:, 'label'] = 1     #중립 : 1\n",
    "fake_news.loc[:, 'label'] = 2        #가짜 : 2\n",
    "\n",
    "use_col_list = ['시각','카테고리','미디어','제목','내용','수정시간','감정표현', 'label']\n",
    "\n",
    "finalDF = pd.concat([neutral_news[use_col_list], real_news[use_col_list], fake_news[use_col_list]], ignore_index = True)\n",
    "finalDF['화나요rate'] = finalDF['감정표현'].apply(lambda x : x[3]/ (sum(x)+0.000001))\n",
    "finalDF['수정시간'] = finalDF['수정시간'].apply(lambda x : 0 if x is np.nan else 1) # 수정 : 1 , 수정안했으면 0\n",
    "\n",
    "finalDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "finalDF.groupby(['카테고리', 'label']).describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "finalDF.groupby(['카테고리', 'label']).count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "finalDF['label'].value_counts().sort_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "finalDF.head(50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "finalDF['내용'].apply(lambda x : x[-30:]).head(50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "finalDF.tail(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# #한글자 단어 중, 유용한 단어거나, 동음이의어가 아닌 하나의 의미만 갖는 단어 추출\n",
    "# import collections\n",
    "\n",
    "# rawContent = finalDF['내용'].apply(lambda x : tokenizer.morphs(x))\n",
    "\n",
    "# tokList = [i for i in rawContent]\n",
    "\n",
    "# collections.Counter([x for sublist in tokList for x in sublist if len(x) == 1]).most_common(400)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "del naverNews, mergeDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "finalDF.to_csv('data/useModelingData_label3.csv', index = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 가장 많이 있는 가짜뉴스 언론사는 ?\n",
    "finalDF[finalDF['label'] == 2]['미디어'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <center>최종 데이터셋을 활용한 Modeling</center>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "from konlpy.tag import Kkma\n",
    "from eunjeon import Mecab\n",
    "\n",
    "from tqdm.notebook import tqdm\n",
    "\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.metrics import classification_report, confusion_matrix, accuracy_score, make_scorer\n",
    "from sklearn.model_selection import train_test_split, cross_val_score, KFold, cross_validate\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "from gensim.models.doc2vec import Doc2Vec, TaggedDocument"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 1. 형태소분석기 활용"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#finalDF = pd.read_csv('useModelingData.csv')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#년 월 데이터만 활용하기로,,\n",
    "finalDF['시각'] = finalDF['시각'].str[:7].apply(lambda x : int(x.replace('.', '')))\n",
    "\n",
    "# 감정표현 string => list 변환\n",
    "finalDF['감정표현'] = finalDF['감정표현'].apply(lambda x : [int(i) for i in x[1:-1].split(', ')])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "finalDF['좋아요'] = finalDF['감정표현'].apply(lambda x : x[0])\n",
    "finalDF['훈훈해요'] = finalDF['감정표현'].apply(lambda x : x[1])\n",
    "finalDF['슬퍼요'] = finalDF['감정표현'].apply(lambda x : x[2])\n",
    "finalDF['화나요'] = finalDF['감정표현'].apply(lambda x : x[3])\n",
    "finalDF['후속기사원해요'] = finalDF['감정표현'].apply(lambda x : x[4])\n",
    "\n",
    "finalDF.tail(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import string\n",
    "\n",
    "string_punctuation = '!\"#$%&\\'()*+,-./:;<=>?[\\\\]^_`{|}~…’'\n",
    "\n",
    "re_tok = re.compile(f'([{string_punctuation}])')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#tokenizer = Okt()\n",
    "tokenizer = Mecab()\n",
    "#tokenizer = Okt()\n",
    "\n",
    "re_tok = re.compile(f'([{string_punctuation}])')   # @ 제외한  특수문자 제거\n",
    "\n",
    "token_title_list = []\n",
    "token_content_list = []\n",
    "\n",
    "\n",
    "\n",
    "# 자주 등장하며, 불용어가 아닌 1글자 단어 List\n",
    "useLen1WordList = [\n",
    "                '년','월','주','문','군','적','핵','명','조','억','또','법','측','뒤','힘','앉',\n",
    "                '글','열','신','률','왔','찾','놓','못','않','없','때','많','딸','뜻','왜','끝',\n",
    "                '곧','액','꿈','뽑','촌','잃','밖','믿','탈','피','편','쉽','첫','본','박','꾼',\n",
    "                   \n",
    "                '文','與','野','美','日','韓','中','靑','北','軍','無','前',\n",
    "                '檢','反','法','黃','秋','發','康','南','朴','男','女','尹','故']\n",
    " \n",
    "\n",
    "\n",
    "finalDF['제목'] = finalDF['제목'].apply(lambda title : re_tok.sub(r'', title))\n",
    "finalDF['내용'] = finalDF['내용'].apply(lambda content : re_tok.sub(r'', content))\n",
    "\n",
    "\n",
    "for text in tqdm(finalDF['제목']):\n",
    "    \n",
    "    token = tokenizer.morphs(text)\n",
    "    token = [t for t in token if (len(t) >= 2) | (t in useLen1WordList)]\n",
    "    token_title_list.append(token)\n",
    "\n",
    "    \n",
    "for text in tqdm(finalDF['내용']):\n",
    "    text = re_tok.sub(r'', text)\n",
    "    \n",
    "    token = tokenizer.morphs(text)\n",
    "    token = [t for t in token if (len(t) >= 2) | (t in useLen1WordList)]\n",
    "    token_content_list.append(token)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Solution1. Doc2vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.models.doc2vec import Doc2Vec, TaggedDocument"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#제목 + 내용\n",
    "total_token = [title + content for title, content in zip(token_title_list, token_content_list)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tagged_data  = [TaggedDocument(d, [i]) for i, d in enumerate(total_token)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 8일때도 좋게나옴\n",
    "\n",
    "doc2vec_model = Doc2Vec(documents = tagged_data,\n",
    "             vector_size = 300, \n",
    "             window = 7, \n",
    "             min_count = 2,\n",
    "             alpha = 0.0025)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "X_text = doc2vec_model.docvecs.vectors_docs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "le_media = LabelEncoder()\n",
    "le_media.fit(finalDF['미디어'])\n",
    "finalDF['미디어'] = le_media.transform(finalDF['미디어'])\n",
    "\n",
    "le_category = LabelEncoder()\n",
    "le_category.fit(finalDF['카테고리'])\n",
    "finalDF['카테고리'] = le_category.transform(finalDF['카테고리'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "finalDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#X_values = finalDF[['시각','수정시간','화나요rate','카테고리_경제', '카테고리_사회', '카테고리_정치']].values\n",
    "X_values = finalDF[['시각','수정시간','화나요rate',\n",
    "                    '좋아요','훈훈해요','슬퍼요','화나요','후속기사원해요','미디어']].values\n",
    "\n",
    "\n",
    "X = np.hstack((X_text, X_values))\n",
    "X = pd.DataFrame(X)   # 추후에,  test 데이터의 index 뽑아내기 위해 데이터프레임으로 변경함\n",
    "\n",
    "y = finalDF['label'].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 삭제\n",
    "\n",
    "# '좋아요',\n",
    "# '훈훈해요',\n",
    "# '슬퍼요',\n",
    "# '화나요',\n",
    "# '후속기사원해요'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### train_test_split "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = X.sample(frac=0.9)\n",
    "y_train = y[X_train.index]\n",
    "\n",
    "X_test = X.drop(X_train.index)\n",
    "y_test = y[X_test.index]\n",
    "\n",
    "print(y_train.shape, y_test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### <center>modeling</center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### model Scoring"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def model_scoring(model, X_test, y_test):\n",
    "    predict = model.predict(X_test)\n",
    "    print('\\n\\n#####classification_report#####')\n",
    "    print(classification_report(y_test, predict))\n",
    "    print('\\n\\n#####confusion Matrix#####')\n",
    "    print(confusion_matrix(y_test, predict))\n",
    "    display(pd.DataFrame(confusion_matrix(y_test, predict)))\n",
    "    print('\\n\\n정확도 : ', accuracy_score(y_test, predict))\n",
    "    \n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### lgbm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import lightgbm\n",
    "\n",
    "clf_lgbm = lightgbm.LGBMClassifier()\n",
    "\n",
    "clf_lgbm.fit(X_train, y_train)\n",
    "\n",
    "model_scoring(clf_lgbm, X_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "finalDF.loc[X_test[np.array([i[2] for i in clf_lgbm.predict_proba(X_test)]) > 0.8].index, : ].sample(frac = 0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### kfold lightgbm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def printKfoldAcc(model, k = 8):\n",
    "    \n",
    "\n",
    "    print ('#### model : {} ####'.format(model))\n",
    "    kfold  = KFold(n_splits = k, shuffle = True)\n",
    "    cv_accuracy=[]\n",
    "\n",
    "    n_iter = 0\n",
    "\n",
    "    # KFold 객체의 split()을 호출하면 폴드 별 학습용, 검증용 테스트의 로우 인덱스를 array로 반환\n",
    "    for train_index, test_index in kfold.split(X.values):\n",
    "        # kfold.split()으로 반환된 인덱스를 이용해 학습용, 검증용 테스트 데이터 추출\n",
    "        X_train, X_test = X.loc[train_index, :], X.loc[test_index, :]\n",
    "        y_train, y_test = y[train_index], y[test_index]\n",
    "\n",
    "        # 학습 및 예측\n",
    "\n",
    "        clf_lgbm.fit(X_train,y_train)\n",
    "        pred = clf_lgbm.predict(X_test)\n",
    "        n_iter += 1\n",
    "\n",
    "        # 반복 시마다 정확도 측정\n",
    "        accuracy = np.round(accuracy_score(y_test, pred), 4) * 100\n",
    "        train_size = X_train.shape[0]\n",
    "        test_size = X_test.shape[0]\n",
    "\n",
    "        print('#{} 검증 정확도 : {} %'.format(n_iter, accuracy))\n",
    "        cv_accuracy.append(accuracy)\n",
    "\n",
    "    print('\\n평균 정확도:{:.2f} % '.format(np.mean(cv_accuracy)))\n",
    "    model_scoring(clf_lgbm, X_test, y_test)\n",
    "    \n",
    "printKfoldAcc(model = clf_lgbm, k = 8)\n",
    "\n",
    "#model_scoring(clf_lgbm, X_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "testing_data = finalDF.loc[X_test.index, :]\n",
    "\n",
    "testing_data['미디어'] = le_media.inverse_transform(testing_data['미디어'])\n",
    "testing_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "finalDF[finalDF['label'] == 2]['미디어'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "le_me"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "le_media.inverse_transform([1, 2, 3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "testing_data[testing_data['미디어'] == '조선일보']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "testing_data[testing_data['미디어'] == 25]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(le_media.inverse_transform([25]))\n",
    "\n",
    "clf_lgbm.predict(testing_data[testing_data['미디어'] == '조선일보'].index, :])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "testing_data[testing_data['미디어'] == '조선일보'].tail(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "clf_lgbm.predict(X_test.loc[testing_data[testing_data['미디어'] == '조선일보'].index, :])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf_lgbm.predict(X_test.loc[testing_data[testing_data['미디어'] == '뉴시스'].index, :])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "finalDF[finalDF['label'] == 1]['미디어'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "finalDF[finalDF['label'] == 2]['미디어'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "X_test[np.array([i[2] for i in clf_lgbm.predict_proba(X_test)]) > 0.8].index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "finalDF.loc[X_test[np.array([i[2] for i in clf_lgbm.predict_proba(X_test)]) > 0.8].index, :].sample(frac = 0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 69 % : 화나요rate / 아무것도없이 초간단 전처리\n",
    "\n",
    "# 72 % : 화나요rate / 한글자 단어들 추가 (문 , 군, 적, 핵, 법, '년','월','주','문','군','적','핵','명','조','억','또','법','측','뒤','힘','앉',)\n",
    "\n",
    "# 75 % : 화나요rate / 모든감정표현5개 / 한글자 단어들 추가\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 삭제가능 ~"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf_rf = RandomForestClassifier()\n",
    "# clf_svc = SVC(kernel='linear', C=1)\n",
    "# clf_NB = GaussianNB()\n",
    "\n",
    "\n",
    "clf_rf.fit(X_train, y_train)\n",
    "# clf_svc.fit(X_train, y_train)\n",
    "# clf_NB.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "model_scoring(clf_rf, X_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# test셋을 통해서 결과 확인\n",
    "threshold = 0.01\n",
    "test_predict_fake_index = np.where(np.array([i[2] for i in clf_rf.predict_proba(X_test)]) > threshold)[0]\n",
    "\n",
    "finalDF.loc[test_predict_fake_index, :].tail(30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "threshold = 0.01\n",
    "train_predict_fake_index = np.where(np.array([i[2] for i in clf_rf.predict_proba(X_train)]) > threshold)[0]\n",
    "\n",
    "finalDF.loc[train_predict_fake_index, :].tail(30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "finalDF['label'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "[i for i in clf_rf.predict_proba(X_test)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### svc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf_svc = SVC(kernel='linear', C=1)\n",
    "\n",
    "clf_svc.fit(X_train.loc[:, :299], y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_scoring(clf_svc, X_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### naive bayies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf_NB = GaussianNB()\n",
    "\n",
    "clf_NB.fit(X_train.loc[:, :299], y_train)\n",
    "\n",
    "model_scoring(clf_NB, X_test.loc[:, :299], y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "finalDF[finalDF['label'] == 0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
